{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **About this dataset**\n",
    "### Naming convention for files -> C_NNNNN_BB_SS_OOOOO_RXX, where:\n",
    "\n",
    "* C = emission context (values: B = brushing; F = waiting for food; I: isolation in an unfamiliar environment);\n",
    "* NNNNN = cat’s unique ID;\n",
    "* BB = breed (values: MC = Maine Coon; EU: European Shorthair);\n",
    "* SS = sex (values: FI = female, intact; FN: female, neutered; MI: male, intact; MN: male, neutered);\n",
    "* OOOOO = cat owner’s unique ID;\n",
    "* R = recording session (values: 1, 2 or 3)\n",
    "* XX = vocalization counter (values: 01..99)ter (values: 01..99)\n",
    " 01..99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T12:59:03.589312Z",
     "iopub.status.busy": "2025-02-21T12:59:03.589032Z",
     "iopub.status.idle": "2025-02-21T12:59:20.298217Z",
     "shell.execute_reply": "2025-02-21T12:59:20.296878Z",
     "shell.execute_reply.started": "2025-02-21T12:59:03.589267Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import IPython.display as ipd\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tensorflow.keras import utils\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from keras.callbacks import Callback\n",
    "from time import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.io import wavfile #read and write soundfile (.wav - time series)\n",
    "from IPython.display import Audio #play file sound in colab\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T12:59:20.302137Z",
     "iopub.status.busy": "2025-02-21T12:59:20.301289Z",
     "iopub.status.idle": "2025-02-21T12:59:20.320889Z",
     "shell.execute_reply": "2025-02-21T12:59:20.319571Z",
     "shell.execute_reply.started": "2025-02-21T12:59:20.302071Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\nicolette\\\\Desktop\\\\CatMe\\\\dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      3\u001b[39m DATA_DIR = \u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mnicolette\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDesktop\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mCatMe\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m'\u001b[39m  \u001b[38;5;66;03m# <- adjust to your folder\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m audio_files = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(audio_files[:\u001b[32m5\u001b[39m])  \u001b[38;5;66;03m# print first 5 files\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\nicolette\\\\Desktop\\\\CatMe\\\\dataset'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = r'C:\\Users\\nicolette\\Desktop\\CatMe\\dataset'  # <- adjust to your folder\n",
    "audio_files = os.listdir(DATA_DIR)\n",
    "print(audio_files[:5])  # print first 5 files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T12:59:20.322996Z",
     "iopub.status.busy": "2025-02-21T12:59:20.322516Z",
     "iopub.status.idle": "2025-02-21T12:59:20.350228Z",
     "shell.execute_reply": "2025-02-21T12:59:20.348877Z",
     "shell.execute_reply.started": "2025-02-21T12:59:20.32295Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 'F' : 'Waiting For Food', 'I' : 'Isolated in unfamiliar Environment', 'B' : 'Brushing'\n",
    "emission_context = {'F' : [], 'I' : [], 'B' : []}\n",
    "\n",
    "# breed (values: MC = Maine Coon; EU: European Shorthair);\n",
    "breed = {'MC' : [], 'EU' : []}\n",
    "\n",
    "for file in audio_files:\n",
    "    split = file.split('_')\n",
    "    if split[0] in emission_context.keys():\n",
    "        emission_context.get(split[0]).append(file)\n",
    "        \n",
    "    if split[2] in breed.keys():\n",
    "        breed.get(split[2]).append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waiting for food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T12:59:20.352857Z",
     "iopub.status.busy": "2025-02-21T12:59:20.351906Z",
     "iopub.status.idle": "2025-02-21T12:59:35.568625Z",
     "shell.execute_reply": "2025-02-21T12:59:35.567324Z",
     "shell.execute_reply.started": "2025-02-21T12:59:20.352805Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wait_food_ex = '/kaggle/input/cat-meow-classification/dataset/dataset/'+ emission_context.get('F')[2]\n",
    "data , sample_rate = librosa.load(wait_food_ex)\n",
    "librosa.load(wait_food_ex)\n",
    "ipd.Audio(wait_food_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T12:59:35.57089Z",
     "iopub.status.busy": "2025-02-21T12:59:35.570195Z",
     "iopub.status.idle": "2025-02-21T12:59:35.992749Z",
     "shell.execute_reply": "2025-02-21T12:59:35.991706Z",
     "shell.execute_reply.started": "2025-02-21T12:59:35.570852Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the audio file using librosa\n",
    "data, sample_rate = librosa.load(wait_food_ex, sr=None)\n",
    "\n",
    "# Calculate time values for the x-axis\n",
    "time = librosa.times_like(data, sr=sample_rate)\n",
    "\n",
    "# Plot the amplitude vs. time graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(time, data, label=\"Waveform\")\n",
    "plt.title(\"Example Sound of Waiting for food\")\n",
    "plt.xlabel(\"Time (seconds)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brushing (being brushed affectionately by the owner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T12:59:35.994698Z",
     "iopub.status.busy": "2025-02-21T12:59:35.994213Z",
     "iopub.status.idle": "2025-02-21T12:59:36.015348Z",
     "shell.execute_reply": "2025-02-21T12:59:36.01415Z",
     "shell.execute_reply.started": "2025-02-21T12:59:35.99465Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "brushing_ex = '/kaggle/input/cat-meow-classification/dataset/dataset/'+ emission_context.get('B')[2]\n",
    "data , sample_rate = librosa.load(brushing_ex)\n",
    "librosa.load(brushing_ex)\n",
    "ipd.Audio(brushing_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T12:59:36.020392Z",
     "iopub.status.busy": "2025-02-21T12:59:36.019611Z",
     "iopub.status.idle": "2025-02-21T12:59:36.322459Z",
     "shell.execute_reply": "2025-02-21T12:59:36.321287Z",
     "shell.execute_reply.started": "2025-02-21T12:59:36.02034Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the audio file using librosa\n",
    "data, sample_rate = librosa.load(brushing_ex, sr=None)\n",
    "\n",
    "# Calculate time values for the x-axis\n",
    "time = librosa.times_like(data, sr=sample_rate)\n",
    "\n",
    "# Plot the amplitude vs. time graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(time, data, label=\"Waveform\")\n",
    "plt.title(\"Example Sound of Brushing\")\n",
    "plt.xlabel(\"Time (seconds)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation in unfamiliar environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T12:59:36.324669Z",
     "iopub.status.busy": "2025-02-21T12:59:36.324177Z",
     "iopub.status.idle": "2025-02-21T12:59:36.344292Z",
     "shell.execute_reply": "2025-02-21T12:59:36.343065Z",
     "shell.execute_reply.started": "2025-02-21T12:59:36.324621Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'emission_context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m isolate_ex = \u001b[33m'\u001b[39m\u001b[33m/kaggle/input/cat-meow-classification/dataset/dataset/\u001b[39m\u001b[33m'\u001b[39m+ \u001b[43memission_context\u001b[49m.get(\u001b[33m'\u001b[39m\u001b[33mI\u001b[39m\u001b[33m'\u001b[39m)[\u001b[32m2\u001b[39m]\n\u001b[32m      2\u001b[39m data , sample_rate = librosa.load(isolate_ex)\n\u001b[32m      3\u001b[39m librosa.load(isolate_ex)\n",
      "\u001b[31mNameError\u001b[39m: name 'emission_context' is not defined"
     ]
    }
   ],
   "source": [
    "isolate_ex = '/kaggle/input/cat-meow-classification/dataset/dataset/'+ emission_context.get('I')[2]\n",
    "data , sample_rate = librosa.load(isolate_ex)\n",
    "librosa.load(isolate_ex)\n",
    "ipd.Audio(isolate_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T12:59:36.346006Z",
     "iopub.status.busy": "2025-02-21T12:59:36.345667Z",
     "iopub.status.idle": "2025-02-21T12:59:36.653347Z",
     "shell.execute_reply": "2025-02-21T12:59:36.652141Z",
     "shell.execute_reply.started": "2025-02-21T12:59:36.345974Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the audio file using librosa\n",
    "data, sample_rate = librosa.load(isolate_ex, sr=None)\n",
    "\n",
    "# Calculate time values for the x-axis\n",
    "time = librosa.times_like(data, sr=sample_rate)\n",
    "\n",
    "# Plot the amplitude vs. time graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(time, data, label=\"Waveform\")\n",
    "plt.title(\"Example Sound of Isolate\")\n",
    "plt.xlabel(\"Time (seconds)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T12:59:36.655678Z",
     "iopub.status.busy": "2025-02-21T12:59:36.655158Z",
     "iopub.status.idle": "2025-02-21T12:59:36.664758Z",
     "shell.execute_reply": "2025-02-21T12:59:36.663033Z",
     "shell.execute_reply.started": "2025-02-21T12:59:36.655627Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def cleanse_audio(file_path, target_sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Cleanses the audio file by performing the following:\n",
    "    - Resampling to the target sample rate\n",
    "    - Normalization\n",
    "    - Trimming silence\n",
    "    \"\"\"\n",
    "    # Load audio file\n",
    "    data, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "    # Trim silence from the beginning and end\n",
    "    data, _ = librosa.effects.trim(data)\n",
    "\n",
    "    # Resample to the target sample rate\n",
    "    if sr != target_sample_rate:\n",
    "        data = librosa.resample(data, orig_sr=sr, target_sr=target_sample_rate)\n",
    "        sr = target_sample_rate\n",
    "\n",
    "    # Normalize the audio to range [-1, 1]\n",
    "    data = librosa.util.normalize(data)\n",
    "\n",
    "    return data, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T12:59:36.667192Z",
     "iopub.status.busy": "2025-02-21T12:59:36.666652Z",
     "iopub.status.idle": "2025-02-21T12:59:36.684301Z",
     "shell.execute_reply": "2025-02-21T12:59:36.682873Z",
     "shell.execute_reply.started": "2025-02-21T12:59:36.667109Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "# extract these features from audio files.\n",
    "\n",
    "# def extract_features(file_path):\n",
    "#     y, sr = librosa.load(file_path, duration=5.0)  # Load the audio file (5 sec duration)\n",
    "#     mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "#     chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "#     mel = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "#     contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "    \n",
    "#     features = np.hstack([\n",
    "#         np.mean(mfcc, axis=1),\n",
    "#         np.mean(chroma, axis=1),\n",
    "#         np.mean(mel, axis=1),\n",
    "#         np.mean(contrast, axis=1)\n",
    "#     ])\n",
    "    # return features\n",
    "\n",
    "def extract_features(file_path, target_sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Cleanses the audio and extracts features (MFCCs, Chroma, Spectral Contrast).\n",
    "    \"\"\"\n",
    "    # Cleanse the audio file\n",
    "    data, sr = cleanse_audio(file_path, target_sample_rate=target_sample_rate)\n",
    "\n",
    "    # Extract MFCCs\n",
    "    mfccs = librosa.feature.mfcc(y=data, sr=sr, n_mfcc=13).mean(axis=1)\n",
    "\n",
    "    # Extract Chroma features\n",
    "    chroma = librosa.feature.chroma_stft(y=data, sr=sr).mean(axis=1)\n",
    "\n",
    "    # Extract Spectral Contrast\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=data, sr=sr).mean(axis=1)\n",
    "\n",
    "    # Combine all features\n",
    "    return np.hstack([mfccs, chroma, spectral_contrast])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T12:59:36.686268Z",
     "iopub.status.busy": "2025-02-21T12:59:36.685923Z",
     "iopub.status.idle": "2025-02-21T13:00:00.563038Z",
     "shell.execute_reply": "2025-02-21T13:00:00.56163Z",
     "shell.execute_reply.started": "2025-02-21T12:59:36.686236Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing data\n",
    "\n",
    "DATA_DIR = '../input/cat-meow-classification/dataset/dataset/'\n",
    "audio_files = os.listdir(DATA_DIR)\n",
    "\n",
    "# Prepare labels\n",
    "emission_context = {'F': 'Waiting For Food', 'I': 'Isolated in unfamiliar Environment', 'B': 'Brushing'}\n",
    "breed_map = {'MC': 'Maine Coon', 'EU': 'European Shorthair'}\n",
    "\n",
    "data = []\n",
    "\n",
    "for file in audio_files:\n",
    "    split = file.split('_')\n",
    "    label_context = split[0]\n",
    "    label_breed = split[2]\n",
    "    \n",
    "    file_path = os.path.join(DATA_DIR, file)\n",
    "    features = extract_features(file_path)\n",
    "    data.append([features, label_context, label_breed])\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data, columns=['features', 'emission_context', 'breed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T13:00:00.569709Z",
     "iopub.status.busy": "2025-02-21T13:00:00.568077Z",
     "iopub.status.idle": "2025-02-21T13:00:00.59663Z",
     "shell.execute_reply": "2025-02-21T13:00:00.594959Z",
     "shell.execute_reply.started": "2025-02-21T13:00:00.569649Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train test spilt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array(df['features'].tolist())\n",
    "y_context = df['emission_context']\n",
    "y_breed = df['breed']\n",
    "\n",
    "# Split for both classification tasks\n",
    "X_train, X_test, y_train_context, y_test_context = train_test_split(X, y_context, test_size=0.2, random_state=42)\n",
    "_, _, y_train_breed, y_test_breed = train_test_split(X, y_breed, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T13:00:00.598355Z",
     "iopub.status.busy": "2025-02-21T13:00:00.597937Z",
     "iopub.status.idle": "2025-02-21T13:00:00.934904Z",
     "shell.execute_reply": "2025-02-21T13:00:00.933879Z",
     "shell.execute_reply.started": "2025-02-21T13:00:00.598312Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train context classifier\n",
    "context_clf = RandomForestClassifier(random_state=42)\n",
    "context_clf.fit(X_train, y_train_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T13:00:00.937015Z",
     "iopub.status.busy": "2025-02-21T13:00:00.936569Z",
     "iopub.status.idle": "2025-02-21T13:00:00.965005Z",
     "shell.execute_reply": "2025-02-21T13:00:00.963885Z",
     "shell.execute_reply.started": "2025-02-21T13:00:00.936968Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluate context classifier\n",
    "y_pred_context = context_clf.predict(X_test)\n",
    "print(\"Emission Context Classification Report:\")\n",
    "print(classification_report(y_test_context, y_pred_context))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1736753,
     "sourceId": 2838478,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
